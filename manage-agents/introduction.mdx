---
title: "Introduction"
description: "Vijil Evaluate is integrated with a number of leading LLM providers."
---

To evaluate serverless LLM endpoints hosted on any of these, follow directions in the [quickstart tutorial](/get-started/quickstart), but with different values for
the model hub and model name.

| Provider | `model_hub` | `model_name` |
|---|---|---|
| OpenAI | `openai` | [List of Models](https://platform.openai.com/docs/models) |
| Together AI | `together` | [List of Models](https://docs.together.ai/docs/serverless-models) |
| Mistral AI | `mistral` | [List of Models](https://docs.mistral.ai/getting-started/models/models_overview) |
| Fireworks AI | `fireworks` | [List of Models](https://fireworks.ai/models?infrastructure=serverless) |
| NVIDIA NIM | `nvidia` | [List of Models](https://docs.api.nvidia.com/nim/reference/llm-apis) |


Vijil also supports a number of other cloud services, giving you the flexibility of evaluating agents accessible through custom endpoints.

<Card
    title="Google Vertex AI"
    horizontal
    href="/manage-agents/integrations/vertex"
    arrow="true"
>
    Learn more about integrating Google Vertex AI
</Card>

<Card
    title="DigitalOcean"
    horizontal
    href="/manage-agents/integrations/digitalocean"
    arrow="true"
>
    Learn more about integrating DigitalOcean
</Card>

<Card
    title="AWS Bedrock"
    horizontal
    href="/manage-agents/integrations/bedrock"
    arrow="true"
>
    Learn more about integrating AWS Bedrock
</Card>
