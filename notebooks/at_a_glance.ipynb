{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vijil Evaluate at-a-glance\n",
    "\n",
    "Vijil Evaluate is Vijilâ€™s flagship evaluation service that enables AI developers evaluate the trustworthiness of an LLM, generative AI application, or agent. Using the Evaluate API, developers can test an AI system under benign and hostile conditions in minutes for reliability, security, and safety.\n",
    "\n",
    "This notebook gives an overview of the major features in Vijil Evaluate through our Python client.\n",
    "\n",
    "## Getting Started\n",
    "To set up your local environment, follow the steps [here](https://docs.vijil.ai/setup.html) to install the Python client and get an API key for the Evaluate Platform.\n",
    "\n",
    "Once the Python client installed, you can instantiate a client class and store an API key for the provider your agent is hosted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vijil import Vijil\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Vijil()\n",
    "client.api_keys.create(\n",
    "    name=\"openai-test\", \n",
    "    model_hub=\"openai\", \n",
    "    api_key=\"sk+++\" # replace with your own api key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to kick off an evaluation job!\n",
    "\n",
    "## Trust Score\n",
    "Vijil evaluates LLMs, AI applications, and agents for task-worthiness (along 5 dimensions of performacne) and trustworthiness (along 8 dimensions of trust). For each dimension of trust, we assessed vulnerability to several attack vectors and propensity to violating areas of compliance. Each attack vector is treated as one evaluation module. Results are summarized into a Vijil Trust Score.\n",
    "\n",
    "The following command kicks off a full trust evaluation job on GPT-4o-mini, setting temperature at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    harnesses=[\"trust_score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep tab on the progress of the job, you can use the `get_status` command or utilize the UI. After the evaluation finishes, use the command again to retrieve the Trust Score for the LLM you tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'c46cffb3-89cc-4def-943d-2ecccccf33b4',\n",
       " 'name': 'OpenAI-gpt-4o-mini-04/04/2025, 15:09:24',\n",
       " 'tags': [''],\n",
       " 'status': 'COMPLETED',\n",
       " 'cause': None,\n",
       " 'total_test_count': 711,\n",
       " 'completed_test_count': 711,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 711,\n",
       " 'completed_response_count': 622,\n",
       " 'error_response_count': 89,\n",
       " 'total_generation_time': '35.000000',\n",
       " 'average_generation_time': '3.5344585091420534',\n",
       " 'score': 0.7845085135235873,\n",
       " 'status_counts': {'probes': {'COMPLETED': 97},\n",
       "  'tests': {'GENERATED': 711},\n",
       "  'responses': {'ERROR': 89, 'COMPLETED': 622}},\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1743804564,\n",
       " 'created_by': '887ef7e6-565b-454e-8dae-277643d6dbab',\n",
       " 'completed_at': 1743804619,\n",
       " 'team_id': 'ef6bdaec-f563-487c-b036-674c912da053',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 73072,\n",
       " 'prompt_tokens': 83806,\n",
       " 'total_tokens': 156878}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get summarized scores at different levels of granularity, you can use `client.evaluations.summarize`. To get prompt-response level logs, you can use `client.evaluations.describe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "For quickly testing an LLM or agent on well-known benchmarks, we have 21 benchmarks available across reliability (e.g. [OpenLLM](https://huggingface.co/open-llm-leaderboard), [OpenLLM v2](https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-2-660cdb7601eba6852431fffc)), security (e.g. [garak](https://garak.ai/), [CyberSecEval 3](https://ai.meta.com/research/publications/cyberseceval-3-advancing-the-evaluation-of-cybersecurity-risks-and-capabilities-in-large-language-models/)), and safety (e.g. [StrongReject](https://arxiv.org/abs/2402.10260), [JailbreakBench](https://arxiv.org/abs/2404.01318)) in Vijil Evaluate.\n",
    "\n",
    "The following command lists available benchmarks in Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARC</td>\n",
       "      <td>ARC (AI2 Reasoning Challenge)</td>\n",
       "      <td>Grade-school science questions (7,787) written...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Semi-automatic Attack Prompts</td>\n",
       "      <td>Use the SAP benchmark (https://arxiv.org/abs/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strong_reject</td>\n",
       "      <td>Strong Reject</td>\n",
       "      <td>Harness for Strong Reject prompts, including o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETHICS_original</td>\n",
       "      <td>ETHICS Benchmark</td>\n",
       "      <td>Probes from the ETHICS benchmark.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realtoxicityprompts</td>\n",
       "      <td>Real Toxicity Prompts</td>\n",
       "      <td>Gets the model to output toxic responses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>socialstigma_qa</td>\n",
       "      <td>Social Stigmas</td>\n",
       "      <td>Using the SocialStigmaQA benchmark to capture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>Measures a model's propensity to reproduce fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>do_not_answer</td>\n",
       "      <td>Do Not Answer</td>\n",
       "      <td>Using the DoNotAnswer benchmark to capture a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Winogrande</td>\n",
       "      <td>Winogrande</td>\n",
       "      <td>An an adversarial and difficult Winograd bench...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>garak</td>\n",
       "      <td>garak</td>\n",
       "      <td>Probes from the open-source garak package.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HarmBench</td>\n",
       "      <td>Evaluate LLMs across harmful behaviours</td>\n",
       "      <td>Using the HarmBench benchmark to evaluate LLMs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MMLU</td>\n",
       "      <td>MMLU</td>\n",
       "      <td>Measures knowledge in zero-shot and few-shot s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GSM8k</td>\n",
       "      <td>GSM8k</td>\n",
       "      <td>Grade school math word problems created by hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bbq</td>\n",
       "      <td>Question Answering Bias</td>\n",
       "      <td>Using the BBQ benchmark, Measure bias in quest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SafeEval</td>\n",
       "      <td>Safe Eval</td>\n",
       "      <td>Using adversarial prompts designed to test the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>Trivial for humans but difficult for state-of-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CyberSecEval3</td>\n",
       "      <td>Cybersecurity Evaluation (CyberSecEval 3)</td>\n",
       "      <td>Using the CyberSecEval 3 benchmark to assess t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>winobias</td>\n",
       "      <td>Professional Bias</td>\n",
       "      <td>Checks if the model has gender stereotypes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jiminycricket</td>\n",
       "      <td>Ethics: Simulation</td>\n",
       "      <td>Evaluates the model's ability to identify the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JBB</td>\n",
       "      <td>JailbreakBench</td>\n",
       "      <td>Using the JailbreakBench benchmark to evaluate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>advglue</td>\n",
       "      <td>Adversarial GLUE</td>\n",
       "      <td>An adversarial version of GLUE benchmark that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       name  \\\n",
       "0                   ARC              ARC (AI2 Reasoning Challenge)   \n",
       "1                   SAP              Semi-automatic Attack Prompts   \n",
       "2         strong_reject                              Strong Reject   \n",
       "3       ETHICS_original                           ETHICS Benchmark   \n",
       "4   realtoxicityprompts                      Real Toxicity Prompts   \n",
       "5       socialstigma_qa                             Social Stigmas   \n",
       "6            TruthfulQA                                 TruthfulQA   \n",
       "7         do_not_answer                              Do Not Answer   \n",
       "8            Winogrande                                 Winogrande   \n",
       "9                 garak                                      garak   \n",
       "10            HarmBench    Evaluate LLMs across harmful behaviours   \n",
       "11                 MMLU                                       MMLU   \n",
       "12                GSM8k                                      GSM8k   \n",
       "13                  bbq                    Question Answering Bias   \n",
       "14             SafeEval                                  Safe Eval   \n",
       "15            HellaSwag                                  HellaSwag   \n",
       "16        CyberSecEval3  Cybersecurity Evaluation (CyberSecEval 3)   \n",
       "17             winobias                          Professional Bias   \n",
       "18        jiminycricket                         Ethics: Simulation   \n",
       "19                  JBB                             JailbreakBench   \n",
       "20              advglue                           Adversarial GLUE   \n",
       "\n",
       "                                          description  \n",
       "0   Grade-school science questions (7,787) written...  \n",
       "1   Use the SAP benchmark (https://arxiv.org/abs/2...  \n",
       "2   Harness for Strong Reject prompts, including o...  \n",
       "3                   Probes from the ETHICS benchmark.  \n",
       "4           Gets the model to output toxic responses.  \n",
       "5   Using the SocialStigmaQA benchmark to capture ...  \n",
       "6   Measures a model's propensity to reproduce fal...  \n",
       "7   Using the DoNotAnswer benchmark to capture a r...  \n",
       "8   An an adversarial and difficult Winograd bench...  \n",
       "9          Probes from the open-source garak package.  \n",
       "10  Using the HarmBench benchmark to evaluate LLMs...  \n",
       "11  Measures knowledge in zero-shot and few-shot s...  \n",
       "12  Grade school math word problems created by hum...  \n",
       "13  Using the BBQ benchmark, Measure bias in quest...  \n",
       "14  Using adversarial prompts designed to test the...  \n",
       "15  Trivial for humans but difficult for state-of-...  \n",
       "16  Using the CyberSecEval 3 benchmark to assess t...  \n",
       "17  Checks if the model has gender stereotypes abo...  \n",
       "18  Evaluates the model's ability to identify the ...  \n",
       "19  Using the JailbreakBench benchmark to evaluate...  \n",
       "20  An adversarial version of GLUE benchmark that ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.harnesses.list(type=\"benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run one or more benchmarks, simply supply the name(s) inside the `harnesses` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0},\n",
    "    harnesses=[\"CyberSecEval3\",\"strong_reject\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Harness\n",
    "\n",
    "Besides a variety of pre-configured harnesses, you can also create your own harnesses in order to obtain a trust score specific to your organization.\n",
    "\n",
    "You can create a custom policy adherence harness that checks whether your model adheres to its system prompt or an organizational policy. To do this, you need a system prompt specified as a string, and an optional organizational policy provided as a `.txt` or `.pdf` file. If you don't provide a policy file, we will create a harness based only on the provided system prompt. To specify that you want a policy adherence harness, you need to specify the `category` argument as `[\"AGENT_POLICY\"]`.\n",
    "\n",
    "The following examples uses the `harnesses.create` function to create a harness to test adherence against the NIST [AI Risk Management](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_creation_job = client.harnesses.create(\n",
    "    harness_name=\"NIST AI RMF harness\",\n",
    "    system_prompt=\"You are a helpful assistant.\", \n",
    "    policy_file_path=\"nist.ai.100-1.pdf\", # download this file from the link above and store it first\n",
    "    category=[\"AGENT_POLICY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` command to know the status of a harness creation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.harnesses.get_status(harness_id=harness_creation_job['harness_config_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `harness_config_version` starts at 1.0.0 for any harness of the given harness name. If you create another harness with the same name, vijil automatically increments the harness version, e.g. from 1.0.0 to 1.0.1. In the above example, we assume that `NIST AI RMF harness` is a new harness name, so we set the version to 1.0.0.\n",
    "\n",
    "Once the harness is created, you can [run an evaluation](evaluations.md#create-an-evaluation) with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.evaluations.create(\n",
    "    harnesses=[harness_creation_job['harness_config_id']],\n",
    "    model_hub=\"your_model_hub\",\n",
    "    model_name=\"your_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For agents with knowledge bases and/or tools attached to them, we also allow creating custom harnesses by specifying pointers to the knowledge base/tools. See [this tutorial](./harness_create.ipynb) for more details on how to do this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
