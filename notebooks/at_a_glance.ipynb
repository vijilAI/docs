{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vijil Evaluate at-a-glance\n",
    "\n",
    "Vijil Evaluate is Vijilâ€™s flagship evaluation service that enables AI developers evaluate the trustworthiness of an LLM, generative AI application, or agent. Using the Evaluate API, developers can test an AI system under benign and hostile conditions in minutes for reliability, security, and safety.\n",
    "\n",
    "This notebook gives an overview of the major features in Vijil Evaluate through our Python client.\n",
    "\n",
    "## Getting Started\n",
    "To set up your local environment, follow the steps [here](https://docs.vijil.ai/setup.html) to install the Python client and get an API key for the Evaluate Platform.\n",
    "\n",
    "Once the Python client installed, you can instantiate a client class and store an API key for the provider your agent is hosted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3bf0b10e-4d49-4d08-a626-3c1693b41922',\n",
       " 'name': 'openai-test',\n",
       " 'hub': 'openai',\n",
       " 'rate_limit_per_interval': 60,\n",
       " 'rate_limit_interval': 10,\n",
       " 'display_value': 'sk*++',\n",
       " 'hub_config': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vijil import Vijil\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Vijil()\n",
    "client.api_keys.create(\n",
    "    name=\"openai-test\", \n",
    "    model_hub=\"openai\", \n",
    "    api_key=\"sk+++\" # replace with your own api key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to kick off an evaluation job!\n",
    "\n",
    "## Trust Score\n",
    "Vijil evaluates LLMs, AI applications, and agents for task-worthiness (along 5 dimensions of performacne) and trustworthiness (along 8 dimensions of trust). For each dimension of trust, we assessed vulnerability to several attack vectors and propensity to violating areas of compliance. Each attack vector is treated as one evaluation module. Results are summarized into a Vijil Trust Score.\n",
    "\n",
    "The following command kicks off a full trust evaluation job on GPT-4o-mini, setting temperature at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 Client Error: Bad Request for url: https://evaluate-api.vijil.ai/v1/evaluations based on response: {\"detail\":\"Resource not found: No harness config found by the name vijil.harnesses.security_Small and version 1.2.44.\"}\n"
     ]
    }
   ],
   "source": [
    "evaluation = client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    harnesses=[\"trust_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep tab on the progress of the job, you can use the `get_status` command or utilize the UI. After the evaluation finishes, use the command again to retrieve the Trust Score for the LLM you tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7eb423be-373c-4b2e-81ec-7abf196b51dd',\n",
       " 'name': 'OpenAI-gpt-4o-mini-04/11/2025, 10:20:35',\n",
       " 'tags': [''],\n",
       " 'status': 'COMPLETED',\n",
       " 'cause': None,\n",
       " 'total_test_count': 711,\n",
       " 'completed_test_count': 711,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 711,\n",
       " 'completed_response_count': 622,\n",
       " 'error_response_count': 89,\n",
       " 'total_generation_time': '96.000000',\n",
       " 'average_generation_time': '5.6540084388185654',\n",
       " 'score': 0.7941188448718497,\n",
       " 'status_counts': {'probes': {'COMPLETED': 97},\n",
       "  'tests': {'GENERATED': 711},\n",
       "  'responses': {'ERROR': 89, 'COMPLETED': 622}},\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1744392036,\n",
       " 'created_by': '887ef7e6-565b-454e-8dae-277643d6dbab',\n",
       " 'completed_at': 1744392138,\n",
       " 'team_id': 'ef6bdaec-f563-487c-b036-674c912da053',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 73507,\n",
       " 'prompt_tokens': 83806,\n",
       " 'total_tokens': 157313}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get summarized scores at different levels of granularity, you can use `client.evaluations.summarize`. To get prompt-response level logs, you can use `client.evaluations.describe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "For quickly testing an LLM or agent on well-known benchmarks, we have 21 benchmarks available across reliability (e.g. [OpenLLM](https://huggingface.co/open-llm-leaderboard), [OpenLLM v2](https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-2-660cdb7601eba6852431fffc)), security (e.g. [garak](https://garak.ai/), [CyberSecEval 3](https://ai.meta.com/research/publications/cyberseceval-3-advancing-the-evaluation-of-cybersecurity-risks-and-capabilities-in-large-language-models/)), and safety (e.g. [StrongReject](https://arxiv.org/abs/2402.10260), [JailbreakBench](https://arxiv.org/abs/2404.01318)) in Vijil Evaluate.\n",
    "\n",
    "The following command lists available benchmarks in Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HarmBench</td>\n",
       "      <td>Evaluate LLMs across harmful behaviours</td>\n",
       "      <td>Using the HarmBench benchmark to evaluate LLMs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM8k</td>\n",
       "      <td>GSM8k</td>\n",
       "      <td>Grade school math word problems created by hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>winobias</td>\n",
       "      <td>Professional Bias</td>\n",
       "      <td>Checks if the model has gender stereotypes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>Trivial for humans but difficult for state-of-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbq</td>\n",
       "      <td>Question Answering Bias</td>\n",
       "      <td>Using the BBQ benchmark, Measure bias in quest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strong_reject</td>\n",
       "      <td>Strong Reject</td>\n",
       "      <td>Harness for Strong Reject prompts, including o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>do_not_answer</td>\n",
       "      <td>Do Not Answer</td>\n",
       "      <td>Using the DoNotAnswer benchmark to capture a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>advglue</td>\n",
       "      <td>Adversarial GLUE</td>\n",
       "      <td>An adversarial version of GLUE benchmark that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Winogrande</td>\n",
       "      <td>Winogrande</td>\n",
       "      <td>An an adversarial and difficult Winograd bench...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>realtoxicityprompts</td>\n",
       "      <td>Real Toxicity Prompts</td>\n",
       "      <td>Gets the model to output toxic responses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jiminycricket</td>\n",
       "      <td>Ethics: Simulation</td>\n",
       "      <td>Evaluates the model's ability to identify the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CyberSecEval3</td>\n",
       "      <td>Cybersecurity Evaluation (CyberSecEval 3)</td>\n",
       "      <td>Using the CyberSecEval 3 benchmark to assess t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ARC</td>\n",
       "      <td>ARC (AI2 Reasoning Challenge)</td>\n",
       "      <td>Grade-school science questions (7,787) written...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Semi-automatic Attack Prompts</td>\n",
       "      <td>Use the SAP benchmark (https://arxiv.org/abs/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MMLU</td>\n",
       "      <td>MMLU</td>\n",
       "      <td>Measures knowledge in zero-shot and few-shot s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JBB</td>\n",
       "      <td>JailbreakBench</td>\n",
       "      <td>Using the JailbreakBench benchmark to evaluate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ETHICS_original</td>\n",
       "      <td>ETHICS Benchmark</td>\n",
       "      <td>Probes from the ETHICS benchmark.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>Measures a model's propensity to reproduce fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>garak</td>\n",
       "      <td>garak</td>\n",
       "      <td>Probes from the open-source garak package.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SafeEval</td>\n",
       "      <td>Safe Eval</td>\n",
       "      <td>Using adversarial prompts designed to test the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>socialstigma_qa</td>\n",
       "      <td>Social Stigmas</td>\n",
       "      <td>Using the SocialStigmaQA benchmark to capture ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       name  \\\n",
       "0             HarmBench    Evaluate LLMs across harmful behaviours   \n",
       "1                 GSM8k                                      GSM8k   \n",
       "2              winobias                          Professional Bias   \n",
       "3             HellaSwag                                  HellaSwag   \n",
       "4                   bbq                    Question Answering Bias   \n",
       "5         strong_reject                              Strong Reject   \n",
       "6         do_not_answer                              Do Not Answer   \n",
       "7               advglue                           Adversarial GLUE   \n",
       "8            Winogrande                                 Winogrande   \n",
       "9   realtoxicityprompts                      Real Toxicity Prompts   \n",
       "10        jiminycricket                         Ethics: Simulation   \n",
       "11        CyberSecEval3  Cybersecurity Evaluation (CyberSecEval 3)   \n",
       "12                  ARC              ARC (AI2 Reasoning Challenge)   \n",
       "13                  SAP              Semi-automatic Attack Prompts   \n",
       "14                 MMLU                                       MMLU   \n",
       "15                  JBB                             JailbreakBench   \n",
       "16      ETHICS_original                           ETHICS Benchmark   \n",
       "17           TruthfulQA                                 TruthfulQA   \n",
       "18                garak                                      garak   \n",
       "19             SafeEval                                  Safe Eval   \n",
       "20      socialstigma_qa                             Social Stigmas   \n",
       "\n",
       "                                          description  \n",
       "0   Using the HarmBench benchmark to evaluate LLMs...  \n",
       "1   Grade school math word problems created by hum...  \n",
       "2   Checks if the model has gender stereotypes abo...  \n",
       "3   Trivial for humans but difficult for state-of-...  \n",
       "4   Using the BBQ benchmark, Measure bias in quest...  \n",
       "5   Harness for Strong Reject prompts, including o...  \n",
       "6   Using the DoNotAnswer benchmark to capture a r...  \n",
       "7   An adversarial version of GLUE benchmark that ...  \n",
       "8   An an adversarial and difficult Winograd bench...  \n",
       "9           Gets the model to output toxic responses.  \n",
       "10  Evaluates the model's ability to identify the ...  \n",
       "11  Using the CyberSecEval 3 benchmark to assess t...  \n",
       "12  Grade-school science questions (7,787) written...  \n",
       "13  Use the SAP benchmark (https://arxiv.org/abs/2...  \n",
       "14  Measures knowledge in zero-shot and few-shot s...  \n",
       "15  Using the JailbreakBench benchmark to evaluate...  \n",
       "16                  Probes from the ETHICS benchmark.  \n",
       "17  Measures a model's propensity to reproduce fal...  \n",
       "18         Probes from the open-source garak package.  \n",
       "19  Using adversarial prompts designed to test the...  \n",
       "20  Using the SocialStigmaQA benchmark to capture ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.harnesses.list(type=\"benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run one or more benchmarks, simply supply the name(s) inside the `harnesses` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '29d604ff-2c3b-4a91-8a79-5fbb6af888bb', 'status': 'CREATED'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0},\n",
    "    harnesses=[\"CyberSecEval3\",\"do_not_answer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Harness\n",
    "\n",
    "Besides a variety of pre-configured harnesses, you can also create your own harnesses in order to obtain a trust score specific to your organization.\n",
    "\n",
    "You can create a custom policy adherence harness that checks whether your model adheres to its system prompt or an organizational policy. To do this, you need a system prompt specified as a string, and an optional organizational policy provided as a `.txt` or `.pdf` file. If you don't provide a policy file, we will create a harness based only on the provided system prompt. To specify that you want a policy adherence harness, you need to specify the `category` argument as `[\"AGENT_POLICY\"]`.\n",
    "\n",
    "The following examples uses the `harnesses.create` function to create a harness to test adherence against the NIST [AI Risk Management](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_creation_job = client.harnesses.create(\n",
    "    name=\"NIST AI RMF harness\",\n",
    "    system_prompt=\"You are a helpful assistant.\", \n",
    "    policy_file_path=\"nist.ai.100-1.pdf\", # download this file from the link above and store it first\n",
    "    category=[\"AGENT_POLICY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` command to know the status of a harness creation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_by': '887ef7e6-565b-454e-8dae-277643d6dbab',\n",
       " 'created_at': 1744392229,\n",
       " 'id': 'e3ca1392-398e-40f2-a6f6-7b2632062316',\n",
       " 'harness_config_id': 'c527607c-58c5-4d78-a3fb-d1b25cd93f7c',\n",
       " 'harness_name': 'NIST AI RMF harness',\n",
       " 'team_id': 'ef6bdaec-f563-487c-b036-674c912da053',\n",
       " 'status': 'COMPLETED',\n",
       " 'status_message': 'Harness NIST AI RMF harness created successfully!',\n",
       " 'agent_system_prompt': 'You are a helpful assistant.',\n",
       " 'started_at': None,\n",
       " 'completed_at': None,\n",
       " 'harness_config_version': '1.0.0'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.harnesses.get_status(harness_id=harness_creation_job['harness_config_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the harness is created, you can [run an evaluation](evaluations.md#create-an-evaluation) with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8125a5f5-5322-4a29-b75e-f0e2aeb21e4e', 'status': 'CREATED'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.create(\n",
    "    harnesses=[harness_creation_job['harness_config_id']],\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    harness_version=\"1.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For agents with knowledge bases (category `KNOWLEDGE_BASE`) and/or tools (CATEGORY `FUNCTION_ROUTE`) attached to them, we also allow creating custom harnesses by specifying pointers to the knowledge base/tools. See [this tutorial](./harness_create.ipynb) for more details on how to do this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
