{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAGs through Vijil Evaluate\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a popular framework of building generative AI applications, where the user can supply queries into a chat interface and get answers back related to a specific knowledge base typically composed od chunked documents.\n",
    "\n",
    "There are two stages of generating an answer through a RAG: \n",
    "1. **Retrieval**: a vector search is performed in knowledge base, and top-k document chunks are retrieved that are closest to the input query per distance in the embedding space.\n",
    "2. **Generation**: Retrieved contexts and the original question are supplied to a Large Language Model (LLM), which generates the final answer for the end user.\n",
    "\n",
    "Vijil Evaluate enables you to evaluate LLMs for RAG capabilities. Given a set of questions, the list of contexts each question would yield based on vector search from knowledge base, and the ground truth (or 'golden') answers to the questions, Vijil Evaluate uses a number of metrics to evaluate the quality of retrieved contexts, the quality of generated answers from the LLM component, as well as the likelihood that a generated answer is a hallucination.\n",
    "\n",
    "Vijil Evaluate currently supports seven metrics to evaluate the generation stage in a RAG pipeline. In this notebook, we show you how to implement these metrics.\n",
    "\n",
    "## Retrieval Metrics\n",
    "\n",
    "To measure the quality of the retrieved contexts, we use two LLM-based metrics. Each produce a score between 0 and 1.\n",
    "\n",
    "- **Contextual Precision**: measures whether the contexts relevant to the input question are ranked higher in the full set of retrieved contexts than irrelevant ones. A higher score indicates greater alignment in ranking.\n",
    "- **Contextual Recall**: measure the extent to which the retrieved contexts align with the golden answers. A higher score indicates greater alignment with the golden answer.\n",
    "\n",
    "Currently, we use `gpt-4o` as the judge LLM in these metrics.\n",
    "\n",
    "## Generation Metrics\n",
    "\n",
    "Our generation metrics are divided into three categories, attempting to measure the LLM in a RAG for different capabilities.\n",
    "\n",
    "### Correctness\n",
    "\n",
    "To measure correctness of the answers generated by a RAG system, we use an LLM-based answer correctness metric that compares the similarity of a generated answer with the ground truth 'golden' answer, and provides a score between 0 and 1. A higher score indicates greater similarity to the golden answer.\n",
    "\n",
    "Besides LLM-based answer correctness, we also offer traditional NLP metrics---BLEU, ROUGE, and BERTScore---for this purpose.\n",
    "\n",
    "### Relevancy\n",
    "\n",
    "Our LLM-based Answer Relevancy metric measures the degree to which the final generated output is relevant to the original input. It produces a score between 0 and 1, higher score indicating higher relevancy.\n",
    "\n",
    "\n",
    "### Hallucination\n",
    "\n",
    "We use an LLM-based Faithfulness metric to measure how much the generated response stays faithful to the retrieved contexts, i.e. the opposite of hallucination. This metric produces scores from 0 to 1, where a higher score means that the response is more faithful to the context (has fewer hallucinations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RAG metrics on your own Inputs and Outputs\n",
    "\n",
    "In the example below, we use the a placeholder dataset to evaluate the reliability of answers in that dataset. To do so, let's load that dataset, and do some preprocessing to convert it to a list of dicts---the format we'll need supply this dataset into Vijil Evaluate in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "qk = pd.read_csv(\"dataset.csv\") # replace with your own dataset\n",
    "\n",
    "# preprocess, assume original columns are named as question, bot_answer, ground_truth, actual_context\n",
    "qk1 = [\n",
    "    {\n",
    "        \"question\": row[\"question\"],\n",
    "        \"response\": row[\"bot_answer\"],\n",
    "        \"ground_truth\": row[\"ground_truth\"],\n",
    "        \"contexts\": ast.literal_eval(row[\"actual_context\"]),\n",
    "    }\n",
    "    for _, row in qk.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate the Vijil Python client. Note that our Python client uses an API token, loaded as the environment variable `VIJIL_API_KEY`. Please make sure you have [fetched an API key](https://docs.vijil.ai/setup.html#authentication-using-api-keys) from the UI and stored it in the env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/miniconda3/envs/py312/lib/python3.12/site-packages/fpdf/__init__.py:39: UserWarning: You have both PyFPDF & fpdf2 installed. Both packages cannot be installed at the same time as they share the same module namespace. To only keep fpdf2, run: pip uninstall --yes pypdf && pip install --upgrade fpdf2\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from vijil import Vijil\n",
    "client = Vijil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics\n",
    "Using the detection endpoint in Vijil Evaluate, you can call the RAG metrics on your dataset. Each of the metrics has a set of required fields that you need to provide. We first store the fields below, then iterate through all metrics to call each of them on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields = {\n",
    "    \"ContextualRecall\": [\"ground_truth\", \"contexts\"],\n",
    "    \"AnswerRelevancy\": [\"question\", \"response\"],\n",
    "    \"ContextualPrecision\": [\"question\", \"ground_truth\", \"contexts\"],\n",
    "    \"Correctness\": [\"question\", \"response\", \"ground_truth\"],\n",
    "    \"Faithfulness\": [\"question\", \"response\", \"contexts\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metric ContextualRecall\n",
      "Calculating metric AnswerRelevancy\n",
      "Calculating metric ContextualPrecision\n",
      "Calculating metric Correctness\n",
      "Calculating metric Faithfulness\n"
     ]
    }
   ],
   "source": [
    "dets = []\n",
    "for metric, fields in required_fields.items():\n",
    "    print(\"Calculating metric\", metric)\n",
    "    detection = client.detections.create(\n",
    "        detector_id=f\"llm.{metric}\",\n",
    "        detector_inputs=[\n",
    "            {\n",
    "                field: row[field]\n",
    "                for field in fields\n",
    "            }\n",
    "            for row in qk1\n",
    "        ]\n",
    "    )\n",
    "    dets.append(detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detection job is created for each metric, and the job IDs are stored in the `dets` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Metrics\n",
    "Now we simply fetch the results of the detection jobs, and summarize the results to get average value of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detector_id\n",
       "AnswerRelevancy        0.752301\n",
       "ContextualPrecision    0.404399\n",
       "ContextualRecall       0.938071\n",
       "Correctness            0.720812\n",
       "Faithfulness           0.860111\n",
       "Name: detector_output, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_dfs = [\n",
    "    client.detections.describe(detection[\"id\"])\n",
    "    for detection in dets\n",
    "]\n",
    "det_df = pd.concat(det_dfs)\n",
    "det_df['detector_id'] = det_df['detector_id'].apply(lambda x: x.split(\".\")[3])\n",
    "det_df['detector_output'] = det_df['detector_output'].apply(lambda x: x['score'])\n",
    "det_df.groupby(\"detector_id\")['detector_output'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the retrieval metrics, the retrieved contexts demonstrate a high degree of recall, but low precision---meaning that while contexts relevant to the ground truth are retrieved consistently, irrelevant contexts get returned with them as well. According to the generation metrics, the RAG-generated answers display a high degree of faithfulness, while being moderately correct and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we append the outputs to the original dataframe and save it to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate per-metric score for row of qk1\n",
    "def score_per_metric(metric, dataset):\n",
    "    # match detector_input by first required field\n",
    "    scores = []\n",
    "    metric_df = det_df[det_df['detector_id'] == metric]\n",
    "    field = required_fields[metric][0]\n",
    "    for row in dataset:\n",
    "        try:\n",
    "            score = metric_df[metric_df['detector_input'].apply(lambda x: x[field] == row[field])]['detector_output'].values[0]\n",
    "        except IndexError:\n",
    "            score = None\n",
    "        scores.append(score.item())\n",
    "    return scores\n",
    "\n",
    "# add scores to qk1\n",
    "for metric in required_fields.keys():\n",
    "    metric_score = score_per_metric(metric, qk1)\n",
    "    for ix, _ in enumerate(qk1):\n",
    "        qk1[ix][metric] = metric_score[ix]\n",
    "        \n",
    "# save as csv\n",
    "qk1_df = pd.DataFrame(qk1)\n",
    "qk1_df.to_csv(\"dataset with scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
