{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Conversations and Tool Calls\n",
    "\n",
    "In this notebook, we are going to demonstrate how you can evaluate AI agents on conversation and tool calling metrics using Vijil Evaluate.\n",
    "\n",
    "## Conversational Metrics\n",
    "\n",
    "We support four conversational metrics.\n",
    "\n",
    "1. **Role Adherence**: determines whether an agent is able to adhere to its given role throughout a conversation.\n",
    "2. **Knowledge Retention**: determines whether an agent is able to retain factual information presented throughout a conversation.\n",
    "3. **Conversation Completeness**: determines whether an agent is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.\n",
    "4. **Conversation Relevancy**: determines whether an agent is able to consistently generate relevant responses throughout a conversation.\n",
    "\n",
    "Below we show how you can evaluate conversations obtained from an agent using these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first instantiate the Vijil client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil(\n",
    "    base_url=\"https://dev-api.vijil.ai/v1\",\n",
    "    api_key=os.getenv(\"VIJIL_API_KEY_DEV\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have loaded a number of canonical conversation histories as the `conversation` test harness. Running this harness asks the agent under test to complete these conversations, then calculates the above metrics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '33a44d17-7de7-4706-8e72-2d9ac0cc9389', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"conversation-evaluation\", # optional\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0}, # optional\n",
    "    harnesses=[\"conversation\"],\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '33a44d17-7de7-4706-8e72-2d9ac0cc9389',\n",
       " 'name': 'OpenAI-gpt-4o-mini',\n",
       " 'tags': [''],\n",
       " 'status': 'COMPLETED',\n",
       " 'cause': None,\n",
       " 'total_test_count': 22,\n",
       " 'completed_test_count': 22,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 22,\n",
       " 'completed_response_count': 22,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': '3.000000',\n",
       " 'average_generation_time': '2.0000000000000000',\n",
       " 'score': 0.40131944444444445,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1732219057,\n",
       " 'created_by': 'f6e0b128-c075-4bc3-91da-34d03fa6c67e',\n",
       " 'completed_at': 1732219113,\n",
       " 'team_id': '00ccc042-1b41-4f02-ae5f-6a09b5e6e844',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 2035,\n",
       " 'prompt_tokens': 1862,\n",
       " 'total_tokens': 3897}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the status changes to `COMPLETE`, you can aggregate the values of all metrics.\n",
    "\n",
    "To do so, we first download all inputs, outputs, and metrics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print out a conversation history, its generated answer, and the metrics that were computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERSATION:\n",
      "{'role': 'user', 'content': \"Hi! I'm planning a trip to Paris next month from the 15th to 22nd\"}\n",
      "{'role': 'assistant', 'content': \"I'll help you plan your Paris trip for next month. For your dates of 15th to 22nd, what specific aspects would you like help with?\"}\n",
      "{'role': 'user', 'content': 'I need help finding a hotel near the Eiffel Tower'}\n",
      "{'role': 'assistant', 'content': \"For your week-long stay from the 15th to 22nd next month, I can help you find hotels near the Eiffel Tower. What's your budget per night?\"}\n",
      "{'role': 'user', 'content': 'Around 200 euros per night'}\n",
      "{'role': 'assistant', 'content': \"I'll look for hotels near the Eiffel Tower within your 200 euros per night budget for your stay from the 15th to 22nd.\"}\n",
      "\n",
      "COMPLETION\n",
      "Here are some hotel options near the Eiffel Tower that typically fall within your budget of around 200 euros per night:\n",
      "\n",
      "1. **Hotel La Comtesse**  \n",
      "   - Located just a short walk from the Eiffel Tower, this hotel offers beautiful views and a cozy atmosphere. Rooms are stylishly decorated, and some have balconies with Eiffel Tower views.\n",
      "\n",
      "2. **Hotel Eiffel Seine**  \n",
      "   - This charming hotel is located near the Seine River and offers easy access to the Eiffel Tower. It features modern amenities and a lovely garden area.\n",
      "\n",
      "3. **Hotel de la Bourdonnais**  \n",
      "   - A boutique hotel\n",
      "\n",
      "METRICS\n",
      "[{'autoredteam.detectors.llm.ConversationKnowledgeRetention': 0.75}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"CONVERSATION:\")\n",
    "for p in df.prompt_list[0]:\n",
    "    print(p)\n",
    "\n",
    "print(f\"\\nCOMPLETION\\n{df.response[0]}\")\n",
    "print(f\"\\nMETRICS\\n{df.score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConversationCompleteness': 0.3333333333333333,\n",
       " 'ConversationKnowledgeRetention': 0.5625,\n",
       " 'ConversationRelevancy': 0.7222222222222222,\n",
       " 'ConversationRoleAdherence': 0.7766666666666666}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "metrics = {}\n",
    "for _, row in df.iterrows():\n",
    "    for met, score in row.score[0].items():\n",
    "        if met not in metrics:\n",
    "            metrics[met] = []\n",
    "        metrics[met].append(score)\n",
    "        \n",
    "# cleanup and average\n",
    "metrics = {met.split(\".\")[3]: mean(metrics[met]) for met in metrics.keys()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the agent under test performed well on conversation relevancy and role adherence, moderately well on knowledge retention, but did rather poorly on completing conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Correctness\n",
    "\n",
    "Our tool correctness metric determines the amount of concordance between a set of expected tools and a set of tools called by a function-calling agent.\n",
    "\n",
    "Let's evaluate a tool calling agent---hosted on DigitalOcean---using this metric. This agent performs a simple arithmetic function called `gonzo_value` on two integers. We first store the credentials of the agent, then kick off an evaluation using an example test harness for this agent comprised of 5 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'efa504cf-ba1e-4edc-97cc-2aafc8d6633b',\n",
       " 'name': 'gonzo-agent',\n",
       " 'hub': 'digitalocean',\n",
       " 'rate_limit_per_interval': 60,\n",
       " 'rate_limit_interval': 60,\n",
       " 'display_value': '',\n",
       " 'hub_config': {'region': None,\n",
       "  'project_id': None,\n",
       "  'client_id': None,\n",
       "  'display_client_secret': None,\n",
       "  'display_access_key': None,\n",
       "  'display_secret_access_key': None,\n",
       "  'agent_id': 'fbb80c9a-9d48-11ef-bf8f-4e013e2ddde4',\n",
       "  'display_agent_key': 'Ro****************************bC'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.api_keys.create(\n",
    "    name=\"gonzo-agent\",\n",
    "    model_hub=\"digitalocean\",\n",
    "    hub_config={\n",
    "        \"agent_id\": os.getenv(\"DO_AGENT_ID\"),\n",
    "        \"agent_key\": os.getenv(\"DO_AGENT_KEY\")\n",
    "    },\n",
    "    rate_limit_per_interval=60, # optional\n",
    "    rate_limit_interval=60 # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '94cc3ca5-9b4a-49e4-bd38-129b3a91ee62', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"gonzo-agent-evaluation\", # optional\n",
    "    api_key_name=\"gonzo-agent\",\n",
    "    model_hub=\"digitalocean\",\n",
    "    model_url=os.getenv(\"DO_AGENT_ENDPOINT\"),\n",
    "    harnesses=[\"tool_correctness\"]\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>avg_detector_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gonzo_value(7,8) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Give me the gonzo value of 1 and 2.</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gonzo(3,4) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the gonzo value of 5 and 6?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>The capital of France is Paris!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  \\\n",
       "0                 gonzo_value(7,8) = ?   \n",
       "1  Give me the gonzo value of 1 and 2.   \n",
       "2                       Gonzo(3,4) = ?   \n",
       "3  What is the gonzo value of 5 and 6?   \n",
       "4       What is the capital of France?   \n",
       "\n",
       "                                            response  avg_detector_score  \n",
       "0  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "1  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "2  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "3  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "4                    The capital of France is Paris!                 1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])\n",
    "df[['prompt','response','avg_detector_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our agent did great! The first four prompts ask the agent to calculate gonzo value of two numbers in different ways. The last prompt asks a question that shouldn't require calling the `gonzo-value` function. In each of the cases, the agent was able to call the correct tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-9BncFrUE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
