{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Conversations and Tool Calls\n",
    "\n",
    "In this notebook, we are going to demonstrate how you can evaluate AI agents on conversation and tool calling metrics using Vijil Evaluate.\n",
    "\n",
    "## Conversational Metrics\n",
    "\n",
    "We support four conversational metrics.\n",
    "\n",
    "1. **Role Adherence**: determines whether an agent is able to adhere to its given role throughout a conversation.\n",
    "2. **Knowledge Retention**: determines whether an agent is able to retain factual information presented throughout a conversation.\n",
    "3. **Conversation Completeness**: determines whether an agent is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.\n",
    "4. **Conversation Relevancy**: determines whether an agent is able to consistently generate relevant responses throughout a conversation.\n",
    "\n",
    "Below we show how you can evaluate conversations obtained from an agent using these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first instantiate the Vijil client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have loaded a number of canonical conversation histories as the `conversation` test harness. Running this harness asks the agent under test to complete these conversations, then calculates the above metrics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '78ba9a24-af57-42fb-917d-f0adff09581d', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"conversation-evaluation\", # optional\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0}, # optional\n",
    "    harnesses=[\"conversation\"],\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '78ba9a24-af57-42fb-917d-f0adff09581d',\n",
       " 'name': 'conversation-evaluation',\n",
       " 'tags': [''],\n",
       " 'status': 'COMPLETED',\n",
       " 'cause': None,\n",
       " 'total_test_count': 22,\n",
       " 'completed_test_count': 22,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 22,\n",
       " 'completed_response_count': 22,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': '7.000000',\n",
       " 'average_generation_time': '2.6818181818181818',\n",
       " 'score': 0.35618055555555556,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1732221884,\n",
       " 'created_by': '887ef7e6-565b-454e-8dae-277643d6dbab',\n",
       " 'completed_at': 1732221919,\n",
       " 'team_id': 'ef6bdaec-f563-487c-b036-674c912da053',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 2063,\n",
       " 'prompt_tokens': 1862,\n",
       " 'total_tokens': 3925}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the status changes to `COMPLETE`, you can aggregate the values of all metrics.\n",
    "\n",
    "To do so, we first download all inputs, outputs, and metrics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print out a conversation history, its generated answer, and the metrics that were computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERSATION:\n",
      "{'role': 'user', 'content': 'Hi! I have something I want to tell you'}\n",
      "{'role': 'assistant', 'content': 'Sure, what is it?'}\n",
      "{'role': 'user', 'content': \"I've a sore throat, what meds should I take?\"}\n",
      "{'role': 'assistant', 'content': 'Have you tried drinking warm water with honey and lemon?'}\n",
      "{'role': 'user', 'content': \"Yes, but it doesn't help\"}\n",
      "{'role': 'assistant', 'content': 'I recommend you see a doctor'}\n",
      "\n",
      "COMPLETION\n",
      "If warm water with honey and lemon isn't helping, you might consider over-the-counter medications such as:\n",
      "\n",
      "1. **Throat lozenges**: These can help soothe the throat.\n",
      "2. **Pain relievers**: Ibuprofen or acetaminophen can help reduce pain and inflammation.\n",
      "3. **Gargling salt water**: This can provide temporary relief.\n",
      "4. **Throat sprays**: Some sprays contain numbing agents that can help alleviate pain.\n",
      "\n",
      "If your sore throat persists for more than a few days, is severe, or is accompanied by other symptoms like fever, difficulty swallowing, or rash, it's\n",
      "\n",
      "METRICS\n",
      "[{'autoredteam.detectors.llm.ConversationCompleteness': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"CONVERSATION:\")\n",
    "for p in df.prompt_list[0]:\n",
    "    print(p)\n",
    "\n",
    "print(f\"\\nCOMPLETION\\n{df.response[0]}\")\n",
    "print(f\"\\nMETRICS\\n{df.score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now summarize the results across all samples and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConversationCompleteness': 0.3333333333333333,\n",
       " 'ConversationKnowledgeRetention': 0.6875,\n",
       " 'ConversationRelevancy': 0.7777777777777778,\n",
       " 'ConversationRoleAdherence': 0.7766666666666666}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "metrics = {}\n",
    "for _, row in df.iterrows():\n",
    "    for met, score in row.score[0].items():\n",
    "        if met not in metrics:\n",
    "            metrics[met] = []\n",
    "        metrics[met].append(score)\n",
    "        \n",
    "# cleanup and average\n",
    "metrics = {met.split(\".\")[3]: mean(metrics[met]) for met in metrics.keys()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the agent under test performed well on conversation relevancy and role adherence, moderately well on knowledge retention, but did rather poorly on completing conversations.\n",
    "\n",
    "To check exact failure modes, you can use the following command to print out all conversations. For brevity we comment it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(0, len(df.prompt_list)):\n",
    "#     print(f\"CONVERSATION\")\n",
    "#     for p in df.prompt_list[idx]:\n",
    "#         print(p)\n",
    "\n",
    "#     print(f\"\\nCOMPLETION\\n{df.response[idx]}\")\n",
    "#     print(f\"\\nMETRICS\\n{df.score[idx]}\")\n",
    "#     print(f\"END\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Correctness\n",
    "\n",
    "Our tool correctness metric determines the amount of concordance between a set of expected tools and a set of tools called by a function-calling agent.\n",
    "\n",
    "Let's evaluate a tool calling agent---hosted on DigitalOcean---using this metric. This agent performs a simple arithmetic function called `gonzo_value` on two integers. We first store the credentials of the agent, then kick off an evaluation using an example test harness for this agent comprised of 5 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a78f7d33-75bd-4166-a432-9777da3294a0',\n",
       " 'name': 'gonzo-agent',\n",
       " 'hub': 'digitalocean',\n",
       " 'rate_limit_per_interval': 60,\n",
       " 'rate_limit_interval': 60,\n",
       " 'display_value': '',\n",
       " 'hub_config': {'region': None,\n",
       "  'project_id': None,\n",
       "  'client_id': None,\n",
       "  'display_client_secret': None,\n",
       "  'display_access_key': None,\n",
       "  'display_secret_access_key': None,\n",
       "  'agent_id': 'fbb80c9a-9d48-11ef-bf8f-4e013e2ddde4',\n",
       "  'display_agent_key': 'Ro****************************bC'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.api_keys.create(\n",
    "    name=\"gonzo-agent\", # optional\n",
    "    model_hub=\"digitalocean\",\n",
    "    hub_config={\n",
    "        \"agent_id\": os.getenv(\"DO_AGENT_ID\"),\n",
    "        \"agent_key\": os.getenv(\"DO_AGENT_KEY\")\n",
    "    },\n",
    "    rate_limit_per_interval=60, # optional\n",
    "    rate_limit_interval=60 # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9ee86b11-8ad1-43db-85b0-766124ff07bb', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"gonzo-agent-evaluation\", # optional\n",
    "    api_key_name=\"gonzo-agent\",\n",
    "    model_hub=\"digitalocean\",\n",
    "    model_url=os.getenv(\"DO_AGENT_ENDPOINT\"),\n",
    "    harnesses=[\"tool_correctness\"]\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>avg_detector_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me the gonzo value of 1 and 2.</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gonzo(3,4) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gonzo_value(7,8) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the gonzo value of 5 and 6?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>The capital of France is Paris!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  \\\n",
       "0  Give me the gonzo value of 1 and 2.   \n",
       "1                       Gonzo(3,4) = ?   \n",
       "2                 gonzo_value(7,8) = ?   \n",
       "3  What is the gonzo value of 5 and 6?   \n",
       "4       What is the capital of France?   \n",
       "\n",
       "                                            response  avg_detector_score  \n",
       "0  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "1  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "2  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "3  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "4                    The capital of France is Paris!                 1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])\n",
    "df[['prompt','response','avg_detector_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our agent did great! The first four prompts ask the agent to calculate gonzo value of two numbers in different ways. The last prompt asks a question that shouldn't require calling the `gonzo-value` function. In each of the cases, the agent was able to call the correct tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-9BncFrUE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
