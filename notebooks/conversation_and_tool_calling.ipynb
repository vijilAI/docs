{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Conversations and Tool Calls\n",
    "\n",
    "In this notebook, we are going to demonstrate how you can evaluate AI agents on conversation and tool calling metrics using Vijil Evaluate.\n",
    "\n",
    "## Conversational Metrics\n",
    "\n",
    "We support four conversational metrics.\n",
    "\n",
    "1. **Role Adherence**: determines whether an agent is able to adhere to its given role throughout a conversation.\n",
    "2. **Knowledge Retention**: determines whether an agent is able to retain factual information presented throughout a conversation.\n",
    "3. **Conversation Completeness**: determines whether an agent is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.\n",
    "4. **Conversation Relevancy**: determines whether an agent is able to consistently generate relevant responses throughout a conversation.\n",
    "\n",
    "Below we show how you can evaluate conversations obtained from an agent using these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first instantiate the Vijil client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have loaded a number of canonical conversation histories as the `conversation` test harness. Running this harness asks the agent under test to complete these conversations, then calculates the above metrics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '78ba9a24-af57-42fb-917d-f0adff09581d', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"conversation-evaluation\", # optional\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0}, # optional\n",
    "    harnesses=[\"conversation\"],\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '78ba9a24-af57-42fb-917d-f0adff09581d',\n",
       " 'name': 'conversation-evaluation',\n",
       " 'tags': [''],\n",
       " 'status': 'COMPLETED',\n",
       " 'cause': None,\n",
       " 'total_test_count': 22,\n",
       " 'completed_test_count': 22,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 22,\n",
       " 'completed_response_count': 22,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': '7.000000',\n",
       " 'average_generation_time': '2.6818181818181818',\n",
       " 'score': 0.35618055555555556,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1732221884,\n",
       " 'created_by': '887ef7e6-565b-454e-8dae-277643d6dbab',\n",
       " 'completed_at': 1732221919,\n",
       " 'team_id': 'ef6bdaec-f563-487c-b036-674c912da053',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 2063,\n",
       " 'prompt_tokens': 1862,\n",
       " 'total_tokens': 3925}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the status changes to `COMPLETE`, you can aggregate the values of all metrics.\n",
    "\n",
    "To do so, we first download all inputs, outputs, and metrics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print out a conversation history, its generated answer, and the metrics that were computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERSATION:\n",
      "{'role': 'user', 'content': 'Hi! I have something I want to tell you'}\n",
      "{'role': 'assistant', 'content': 'Sure, what is it?'}\n",
      "{'role': 'user', 'content': \"I've a sore throat, what meds should I take?\"}\n",
      "{'role': 'assistant', 'content': 'Have you tried drinking warm water with honey and lemon?'}\n",
      "{'role': 'user', 'content': \"Yes, but it doesn't help\"}\n",
      "{'role': 'assistant', 'content': 'I recommend you see a doctor'}\n",
      "\n",
      "COMPLETION\n",
      "If warm water with honey and lemon isn't helping, you might consider over-the-counter medications such as:\n",
      "\n",
      "1. **Throat lozenges**: These can help soothe the throat.\n",
      "2. **Pain relievers**: Ibuprofen or acetaminophen can help reduce pain and inflammation.\n",
      "3. **Gargling salt water**: This can provide temporary relief.\n",
      "4. **Throat sprays**: Some sprays contain numbing agents that can help alleviate pain.\n",
      "\n",
      "If your sore throat persists for more than a few days, is severe, or is accompanied by other symptoms like fever, difficulty swallowing, or rash, it's\n",
      "\n",
      "METRICS\n",
      "[{'autoredteam.detectors.llm.ConversationCompleteness': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"CONVERSATION:\")\n",
    "for p in df.prompt_list[0]:\n",
    "    print(p)\n",
    "\n",
    "print(f\"\\nCOMPLETION\\n{df.response[0]}\")\n",
    "print(f\"\\nMETRICS\\n{df.score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now summarize the results across all samples and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConversationCompleteness': 0.3333333333333333,\n",
       " 'ConversationKnowledgeRetention': 0.6875,\n",
       " 'ConversationRelevancy': 0.7777777777777778,\n",
       " 'ConversationRoleAdherence': 0.7766666666666666}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "metrics = {}\n",
    "for _, row in df.iterrows():\n",
    "    for met, score in row.score[0].items():\n",
    "        if met not in metrics:\n",
    "            metrics[met] = []\n",
    "        metrics[met].append(score)\n",
    "        \n",
    "# cleanup and average\n",
    "metrics = {met.split(\".\")[3]: mean(metrics[met]) for met in metrics.keys()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the agent under test performed well on conversation relevancy and role adherence, moderately well on knowledge retention, but did rather poorly on completing conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Correctness\n",
    "\n",
    "Our tool correctness metric determines the amount of concordance between a set of expected tools and a set of tools called by a function-calling agent.\n",
    "\n",
    "Let's evaluate a tool calling agent---hosted on DigitalOcean---using this metric. This agent performs a simple arithmetic function called `gonzo_value` on two integers. We first store the credentials of the agent, then kick off an evaluation using an example test harness for this agent comprised of 5 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a78f7d33-75bd-4166-a432-9777da3294a0',\n",
       " 'name': 'gonzo-agent',\n",
       " 'hub': 'digitalocean',\n",
       " 'rate_limit_per_interval': 60,\n",
       " 'rate_limit_interval': 60,\n",
       " 'display_value': '',\n",
       " 'hub_config': {'region': None,\n",
       "  'project_id': None,\n",
       "  'client_id': None,\n",
       "  'display_client_secret': None,\n",
       "  'display_access_key': None,\n",
       "  'display_secret_access_key': None,\n",
       "  'agent_id': 'fbb80c9a-9d48-11ef-bf8f-4e013e2ddde4',\n",
       "  'display_agent_key': 'Ro****************************bC'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.api_keys.create(\n",
    "    name=\"gonzo-agent\", # optional\n",
    "    model_hub=\"digitalocean\",\n",
    "    hub_config={\n",
    "        \"agent_id\": os.getenv(\"DO_AGENT_ID\"),\n",
    "        \"agent_key\": os.getenv(\"DO_AGENT_KEY\")\n",
    "    },\n",
    "    rate_limit_per_interval=60, # optional\n",
    "    rate_limit_interval=60 # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9ee86b11-8ad1-43db-85b0-766124ff07bb', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    name=\"gonzo-agent-evaluation\", # optional\n",
    "    api_key_name=\"gonzo-agent\",\n",
    "    model_hub=\"digitalocean\",\n",
    "    model_url=os.getenv(\"DO_AGENT_ENDPOINT\"),\n",
    "    harnesses=[\"tool_correctness\"]\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>avg_detector_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me the gonzo value of 1 and 2.</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gonzo(3,4) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gonzo_value(7,8) = ?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the gonzo value of 5 and 6?</td>\n",
       "      <td>[{'id': 'n/a', 'function': {'name': 'gonzo-val...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>The capital of France is Paris!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  \\\n",
       "0  Give me the gonzo value of 1 and 2.   \n",
       "1                       Gonzo(3,4) = ?   \n",
       "2                 gonzo_value(7,8) = ?   \n",
       "3  What is the gonzo value of 5 and 6?   \n",
       "4       What is the capital of France?   \n",
       "\n",
       "                                            response  avg_detector_score  \n",
       "0  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "1  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "2  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "3  [{'id': 'n/a', 'function': {'name': 'gonzo-val...                 1.0  \n",
       "4                    The capital of France is Paris!                 1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])\n",
    "df[['prompt','response','avg_detector_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our agent did great! The first four prompts ask the agent to calculate gonzo value of two numbers in different ways. The last prompt asks a question that shouldn't require calling the `gonzo-value` function. In each of the cases, the agent was able to call the correct tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-9BncFrUE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
