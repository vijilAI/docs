{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAGs through Vijil Evaluate\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a popular framework of building generative AI applications, where the user can supply queries into a chat interface and get answers back related to a specific knowledge base typically composed od chunked documents.\n",
    "\n",
    "There are two stages of generating an answer through a RAG: \n",
    "1. **Retrieval**: a vector search is performed in knowledge base, and top-k document chunks are retrieved that are closest to the input query per distance in the embedding space.\n",
    "2. **Generation**: Retrieved contexts and the original question are supplied to a Large Language Model (LLM), which generates the final answer for the end user.\n",
    "\n",
    "Vijil Evaluate enables you to evaluate LLMs for RAG capabilities. Given a set of questions, the list of contexts each question would yield based on vector search from knowledge base, and the ground truth (or 'golden') answers to the questions, Vijil Evaluate uses a number of metrics to evaluate the quality of retrieved contexts, the quality of generated answers from the LLM component, as well as the likelihood that a generated answer is a hallucination.\n",
    "\n",
    "Vijil Evaluate currently supports seven metrics to evaluate the generation stage in a RAG pipeline. In this notebook, we show you how to implement these metrics.\n",
    "\n",
    "## Retrieval Metrics\n",
    "\n",
    "To measure the quality of the retrieved contexts, we use two LLM-based metrics. Each produce a score between 0 and 1.\n",
    "\n",
    "- **Contextual Precision**: measures whether the contexts relevant to the input question are ranked higher in the full set of retrieved contexts than irrelevant ones. A higher score indicates greater alignment in ranking.\n",
    "- **Contextual Recall**: measure the extent to which the retrieved contexts align with the golden answers. A higher score indicates greater alignment with the golden answer.\n",
    "\n",
    "Currently, we use `gpt-4o` as the judge LLM in these metrics.\n",
    "\n",
    "## Generation Metrics\n",
    "\n",
    "Our generation metrics are divided into three categories, attempting to measure the LLM in a RAG for different capabilities.\n",
    "\n",
    "### Correctness\n",
    "\n",
    "To measure correctness of the LLM-generated answers, we use the following traditional NLP metrics.\n",
    "\n",
    "- BLEU\n",
    "- METEOR\n",
    "- BERTScore\n",
    "\n",
    "Each of them compares the similarity of an LLM-generated answer with the ground truth 'golden' answer, and provides a score between 0 and 1. A higher score indicates greater similarity to the golden answer.\n",
    "\n",
    "### Relevancy\n",
    "\n",
    "Our LLM-based Answer Relevancy metric measures the degree to which the final generated output is relevant to the original input. It produces a score between 0 and 1, higher score indicating higher relevancy.\n",
    "\n",
    "\n",
    "### Hallucination\n",
    "\n",
    "We use an LLM-based Faithfulness metric to measure how much the generated response stays faithful to the retrieved contexts, i.e. the opposite of hallucination. This metric produces scores from 0 to 1, where a higher score means that the response is more faithful to the context (has fewer hallucinations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Domain-specific Question Answering\n",
    "\n",
    "In the example below, we use the [FinanceBench](https://huggingface.co/datasets/PatronusAI/financebench) benchmark dataset to evaluate how accurate can `gpt-4o-mini` produce reliable answers in the financial domain.\n",
    "\n",
    "Note that our Python client uses an API token, loaded as the environment variable `VIJIL_API_KEY`. Please make sure you have [fetched an API key](https://docs.vijil.ai/setup.html#authentication-using-api-keys) from the UI and stored it in the env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/miniconda3/envs/py312/lib/python3.12/site-packages/fpdf/__init__.py:39: UserWarning: You have both PyFPDF & fpdf2 installed. Both packages cannot be installed at the same time as they share the same module namespace. To only keep fpdf2, run: pip uninstall --yes pypdf && pip install --upgrade fpdf2\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# !pip install vijil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already loaded FinanceBench as an evaluation harness in Vijil Evaluate. Now we simply create an evaluation of the given LLM on this harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '726285f8-ec61-431b-9673-8ec722707031', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation\n",
    "evaluation = client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0},\n",
    "    harnesses=[\"financebench\"],\n",
    ")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '726285f8-ec61-431b-9673-8ec722707031',\n",
       " 'name': 'openai-gpt-4o-mini',\n",
       " 'tags': [''],\n",
       " 'status': 'IN_PROGRESS',\n",
       " 'total_test_count': 20,\n",
       " 'completed_test_count': 0,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 0,\n",
       " 'completed_response_count': 0,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': None,\n",
       " 'average_generation_time': None,\n",
       " 'score': None,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1728673801,\n",
       " 'created_by': 'f6e0b128-c075-4bc3-91da-34d03fa6c67e',\n",
       " 'completed_at': None,\n",
       " 'team_id': '00ccc042-1b41-4f02-ae5f-6a09b5e6e844',\n",
       " 'restart_count': 0,\n",
       " 'metadata': None,\n",
       " 'completion_tokens': 0,\n",
       " 'prompt_tokens': 0,\n",
       " 'total_tokens': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the status changes to `COMPLETE`, you can aggregate the values of all metrics.\n",
    "\n",
    "To do so, we first download all inputs, outputs, and metrics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.evaluations.describe(evaluation_id=evaluation[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print out a question, its generated answer, and the metrics that were computed for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION\n",
      "By how much did Pepsico increase its unsecured five year revolving credit agreement on May 26, 2023?\n",
      "ANSWER\n",
      "PepsiCo increased its unsecured five year revolving credit agreement by $400,000,000 on May 26, 2023, going from $3,800,000,000 to $4,200,000,000.\n",
      "METRICS\n",
      "[{'autoredteam.detectors.nlp.BLEU': 0.0}, {'autoredteam.detectors.nlp.METEOR': 0.32258064516129037}, {'autoredteam.detectors.nlp.BERTScore': 0.5262122983517854}, {'autoredteam.detectors.llm.AnswerRelevancy': 1.0}, {'autoredteam.detectors.llm.ContextualPrecision': 1.0}, {'autoredteam.detectors.llm.ContextualRecall': 1.0}, {'autoredteam.detectors.llm.Faithfulness': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "print(f\"QUESTION\\n{ast.literal_eval(df.triggers[0][0])['question']}\")\n",
    "print(f\"ANSWER\\n{df.response[0]}\")\n",
    "print(f\"METRICS\\n{df.score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now aggregate across all samples in the dataset and compute the average metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU                   0.063651\n",
       "METEOR                 0.299480\n",
       "BERTScore              0.501233\n",
       "AnswerRelevancy        0.744167\n",
       "ContextualPrecision    0.975000\n",
       "ContextualRecall       0.766667\n",
       "Faithfulness           0.733975\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# extract metric names\n",
    "metric_names = [list(met.keys())[0] for met in df.score[0]]\n",
    "\n",
    "# flatten the metrics\n",
    "metrics = {}\n",
    "for met in metric_names:\n",
    "    metrics[met] = []\n",
    "for _, row in df.iterrows():\n",
    "    for idx, met in enumerate(metric_names):\n",
    "        metrics[met].append(row[\"score\"][idx][met])\n",
    "        \n",
    "# cleanup and average\n",
    "metrics = {met.split(\".\")[3]: metrics[met] for met in metric_names}\n",
    "metrics_df = pd.DataFrame(metrics).mean()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BLEU and METEOR scores are very low, there is a moderate amount (50%) of semantic overlap between the generated responses and golden answers, as per BERTScore. Per, `AnswerRelevancy`, the generated answers are moderately relevant to the input question. Contextual precision is high, since the benchmark dataset do not contain any contexts relevant to the golden answers. Contextual recall is around 77%, indicating that sometimes the generated answer may not include all information in the contexts. As per the Faithfulness metric, the generated responses may have some hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you are developing your own RAG system and have your own dataset of prompts, contexts, and desired responses at hand, you can use Vijil Evaluate to similarly evaluate your system on that dataset. Please reach out to contact@vijil.ai to know more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
