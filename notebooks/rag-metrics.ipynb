{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAGs through Vijil Evaluate\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a popular framework of building generative AI applications, where the user can supply queries into a chat interface and get answers back related to a specific knowledge base typically composed od chunked documents.\n",
    "\n",
    "There are two stages of generating an answer through a RAG: \n",
    "1. **Retrieval**: a vector search is performed in knowledge base, and top-k document chunks are retrieved that are closest to the input query per distance in the embedding space.\n",
    "2. **Generation**: Retrieved contexts and the original question are supplied to a Large Language Model (LLM), which generates the final answer for the end user.\n",
    "\n",
    "Vijil Evaluate enables you to evaluate LLMs for RAG capabilities. Given a set of questions, the list of contexts each question would yield based on vector search from knowledge base, and the ground truth (or 'golden') answers to the questions, Vijil Evaluate uses a number of metrics to evaluate the quality of generated answers from the LLM component, as well as the likelihood that a generated answer is a hallucination.\n",
    "\n",
    "Vijil Evaluate currently supports four metrics to evaluate the generation stage in a RAG pipeline. In this notebook, we show you how to implement these metrics.\n",
    "\n",
    "## Correctness Metrics\n",
    "\n",
    "To measure correctness of the LLM-generated answers, we use the following traditional NLP metrics.\n",
    "\n",
    "- BLEU\n",
    "- METEOR\n",
    "- BERTScore\n",
    "\n",
    "Each of them compares the similarity of an LLM-generated answer with the ground truth 'golden' answer, and provides a score between 0 and 1. A higher score indicates greater similarity to the golden answer.\n",
    "\n",
    "\n",
    "## Hallucination Metrics\n",
    "\n",
    "We use the [HHEM](https://huggingface.co/vectara/hallucination_evaluation_model) Hallucination Evaluation classifier to measure the propensity that the generated response is hallucinated. To do so, we supply the generated response and concatenated contexts to the model, and take the output probability that the two input strings are consistent with each other as the final score. HHEM produces scores from 0 to 1, where a higher score means that the response is more faithful to the context (has fewer hallucinations).\n",
    "\n",
    "## Evaluating Domain-specific Question Answering\n",
    "\n",
    "In the example below, we use the [financebench](https://huggingface.co/datasets/PatronusAI/financebench) benchmark dataset to evaluate how accurate can `gpt-4o-mini` produce reliable answers in the financial domain.\n",
    "\n",
    "We have already loaded the benchmark as an evaluation harness in Vijil Evaluate. Now we simply create an evaluation of the given LLM on this harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '395f15be-b0f0-4060-957b-1c325eaa9f89', 'status': 'CREATED'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install vijil\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil()\n",
    "\n",
    "# create the evaluation\n",
    "client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\"temperature\": 0},\n",
    "    harnesses=[\"financebench\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '395f15be-b0f0-4060-957b-1c325eaa9f89',\n",
       " 'status': 'IN_PROGRESS',\n",
       " 'total_test_count': 600,\n",
       " 'completed_test_count': 600,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 600,\n",
       " 'completed_response_count': 300,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': '28.000000',\n",
       " 'average_generation_time': '2.9250000000000000',\n",
       " 'score': None,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o',\n",
       " 'url': '',\n",
       " 'created_at': 1726530926,\n",
       " 'created_by': '7ad3420b-2c22-4f07-a8f4-ab6c334c1421',\n",
       " 'completed_at': None,\n",
       " 'team_id': 'jGMHAXYmQ3RO59ebD0Bruupx8ClqhGTC@clients'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status('395f15be-b0f0-4060-957b-1c325eaa9f89')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the evaluation finishes, you can use the following code to obtain the four metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLEU</td>\n",
       "      <td>0.0445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HHEM</td>\n",
       "      <td>0.1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METEOR</td>\n",
       "      <td>0.3284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERTScore</td>\n",
       "      <td>0.5008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric   score\n",
       "2       BLEU  0.0445\n",
       "3       HHEM  0.1242\n",
       "4     METEOR  0.3284\n",
       "5  BERTScore  0.5008"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.evaluations.summarize('395f15be-b0f0-4060-957b-1c325eaa9f89')\n",
    "df = df[df.level==\"probe\"]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({\n",
    "    \"metric\": df.level_name.apply(lambda s: s.replace(\"FinanceBench, metric \", \"\")),\n",
    "    \"score\": df.score.apply(lambda s: (100-s)/100)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BLEU and METEOR scores are very low, there is a moderate amount (50%) of semantic overlap between the generated responses and golden answers, as per BERTScore. As per HHEM, the generated responses may involve significant hallucinations as their average is closer to 0 than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you are developing your own RAG system and have your own dataset of prompts, contexts, and desired responses at hand, you can use Vijil Evaluate to similarly evaluate your system on that dataset. Please reach out to contact@vijil.ai to know more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-9BncFrUE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
