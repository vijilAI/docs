{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing OWASP LLM Top 10 Vulnerabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Vijil, we have scoured latest AI security and red teaming research to gather a large volume of prompts relevant to the vulnerabilities in [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/). We use a number of probes that group these prompts, and one or more probes are mapped to vulnerability categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first instantiate the Vijil client and connect to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vijil\n",
    "\n",
    "# import and instantiate the client\n",
    "from vijil import Vijil\n",
    "client = Vijil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an evaluation with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6a6b903b-040b-44e8-9131-a0a862de0879', 'status': 'CREATED'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the evaluation\n",
    "client.evaluations.create(\n",
    "    model_hub=\"openai\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    # model_hub=\"together\",\n",
    "    # model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    model_params={\"temperature\": 0},\n",
    "    harnesses=[\"owasp\"],\n",
    "    harness_params={\"sample_size\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above code, you can easily swap out OpenAI/gpt-4o-mini for Llama-3.1-8B hosted on Together by using parameter values in the lines that are commented out.\n",
    "\n",
    "You can use the `get_status` method to keep track of the progress of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ca7f5c2c-f932-47ed-bfb6-949e3bacc3c9',\n",
       " 'status': 'COMPLETED',\n",
       " 'total_test_count': 701,\n",
       " 'completed_test_count': 701,\n",
       " 'error_test_count': 0,\n",
       " 'total_response_count': 701,\n",
       " 'completed_response_count': 701,\n",
       " 'error_response_count': 0,\n",
       " 'total_generation_time': '38.000000',\n",
       " 'average_generation_time': '6.3894436519258203',\n",
       " 'score': 0.5917669709517536,\n",
       " 'hub': 'openai',\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'url': '',\n",
       " 'created_at': 1727402922,\n",
       " 'created_by': 'f6e0b128-c075-4bc3-91da-34d03fa6c67e',\n",
       " 'completed_at': 1727402966,\n",
       " 'team_id': '00ccc042-1b41-4f02-ae5f-6a09b5e6e844',\n",
       " 'restart_count': 0,\n",
       " 'is_lite': False,\n",
       " 'metadata': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.evaluations.get_status('ca7f5c2c-f932-47ed-bfb6-949e3bacc3c9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the evaluation finishes, you can use the following code to obtain the four metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM01: Prompt Injection</td>\n",
       "      <td>68.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM02: Insecure Output Handling</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM05: Supply Chain Vulnerabilities</td>\n",
       "      <td>37.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM06: Sensitive Information Disclosure</td>\n",
       "      <td>78.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM08: Excessive Agency</td>\n",
       "      <td>51.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM09: Overreliance</td>\n",
       "      <td>51.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM10: Model Theft</td>\n",
       "      <td>83.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                level_name  score\n",
       "6                  LLM01: Prompt Injection  68.26\n",
       "1          LLM02: Insecure Output Handling  44.00\n",
       "2      LLM05: Supply Chain Vulnerabilities  37.50\n",
       "3  LLM06: Sensitive Information Disclosure  78.24\n",
       "7                  LLM08: Excessive Agency  51.88\n",
       "4                      LLM09: Overreliance  51.21\n",
       "5                       LLM10: Model Theft  83.15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.evaluations.summarize('ca7f5c2c-f932-47ed-bfb6-949e3bacc3c9')\n",
    "df[df.level==\"scenario\"].sort_values(\"level_name\")[['level_name','score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vijil Evaluate covers 7 of the 10 OWASP Top 10 vulnerabilities. Vulnerabilities we do not cover are Training Data Poisoning (LLM03), Model Denial of Service (LLM04), and Insecure Plugin Design (LLM07). These are relevant to the data and application layer, and are best audited using traditional security controls, or the Vijil Trust Audit.\n",
    "\n",
    "If you are interested in the OWASP LLM Top 10 evaluation or Vijil Trust Audit for your genAI application, reach out to contact@vijil.ai to get started."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-9BncFrUE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
